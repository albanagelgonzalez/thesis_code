{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc610ab",
   "metadata": {},
   "source": [
    "# Detaxizer Notebook\n",
    "\n",
    "The notebook provides a step-by-step explanation of the `nf-core/detaxizer` pipeline.\n",
    "\n",
    "To familiarize yourself with the nf-core piplines [click here](https://nf-co.re/). **Detaxizer** processes raw metagenomic sequencing data (FASTQ format) and enables the detection or optional removal of specific taxa.\n",
    "\n",
    "To achieve it, various tools such as bbduk, kraken2 and blastn are applied. In this notebook, the necessary libraries and automated sample sheets (which are the processed input for detaxizer) are described for easy reproducibility. \n",
    "\n",
    "For further information check out the official repository: https://github.com/nf-core/detaxizer/tree/1.0.0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b373f4",
   "metadata": {},
   "source": [
    "## Important setup:\n",
    "\n",
    "In order to use this pipline it is necessary to have conda installed - [click here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html). \n",
    "\n",
    "If you are running this notebook on the M3 cluster, you should already have conda installed.\n",
    "\n",
    "The second step is to set up a Nextflow environment, which is required for the pipeline. This environment installs all necessary software dependencies in isolation, preventing conflicts and ensuring reproducibility of the workflow.\n",
    "\n",
    "Every time you wish to install a new library, do it in an environment by creating it with \"conda create --name <insert_name>\". Access the environment with \"conda activate <env_name>\".\n",
    "\n",
    "Last but not least, update your `~/.bashrc` file. If you are part of the M3 team you can add an alias such as `alias m3='ssh username@l1.m3c.uni-tuebingen.de'` which is a shortcut for accessing the server.\n",
    "\n",
    "Also, set a common directory for storing downloaded nf-core pipelines, and adjust the path to match your system. An example would be:\n",
    "```\n",
    "export NXF_SINGULARITY_CACHEDIR=\"/mnt/lustre/groups/maier/YOUR_M3HPC_USERNAME/bin/nf-core\"\n",
    "```\n",
    "\n",
    "To run multiple tasks at the same time, I recommend using the task manager `Screen`. This way, even if you lose your HPC connection, your sessions will continue running.\n",
    "\n",
    "You might encounter problems when running an nf-core pipeline with your setup. One common mistake is having the wrong version of the pipeline, which clashes with your settings. More is detailed in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39324bef",
   "metadata": {},
   "source": [
    "## Libraries to load beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32a1f1b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.3.3”\n",
      "Warning message:\n",
      "“package ‘stringr’ was built under R version 4.3.3”\n",
      "Warning message:\n",
      "“package ‘forcats’ was built under R version 4.3.3”\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "# Package loading:\n",
    "library(tidyverse)\n",
    "library(conflicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcf053",
   "metadata": {},
   "source": [
    "## Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d30a1c0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m\u001b[90m[conflicted]\u001b[39m Will prefer \u001b[1m\u001b[34mdplyr\u001b[39m\u001b[22m::filter over any other package.\n",
      "\u001b[1m\u001b[22m\u001b[90m[conflicted]\u001b[39m Will prefer \u001b[1m\u001b[34mdplyr\u001b[39m\u001b[22m::lag over any other package.\n"
     ]
    }
   ],
   "source": [
    "# Specifying preferences to solve conflicts\n",
    "conflicts_prefer(dplyr::filter)\n",
    "conflicts_prefer(dplyr::lag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce5a4c",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfaca4a8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Set the base directory. Place to test small data sets.\n",
    "base_dir = \"/mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/raw_data\"\n",
    "\n",
    "# Output directory\n",
    "out_dir = \"/mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output\"\n",
    "dir.create(out_dir)\n",
    "\n",
    "# Sheet directory\n",
    "sheet_dir = file.path(out_dir, \"sheets\")\n",
    "dir.create(sheet_dir)\n",
    "\n",
    "# Pipeline output\n",
    "output_detax = file.path(out_dir, \"output_detax\")\n",
    "dir.create(output_detax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a511b6",
   "metadata": {},
   "source": [
    "## Automate the samplesheet.csv\n",
    "\n",
    "The following code sets a base directory (`base_dir`) where the input data is stored. All the files ending with `.fastq.gz` in the directory and also subfolders are listed and separated in `forward_reads` (contains the R1 label in the filename) and `reverse_reads`(which contains R2). \n",
    "\n",
    "Then the sample ID is extracted (to identify each sequence) and it is applied to the forward and reverse reads. The read lists are aligned to ensure matching sample order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb0ba7c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Makes a list of all files ending with fastq.gz files in the \"raw_data\" directory, iterating through all subdirectories\n",
    "data_fastqs = list.files(path = base_dir, \n",
    "  pattern = \"\\\\.fastq\\\\.gz$\", \n",
    "  recursive = TRUE, \n",
    "  full.names = TRUE)\n",
    "\n",
    "# Since column \"short_reads_fastq_1\" needs the R1 files and column \"short_reads_fastq_2\" needs the R2 files they are filterd separately\n",
    "forward_reads = data_fastqs[grepl(\"R1\", data_fastqs)]\n",
    "reverse_reads = data_fastqs[grepl(\"R2\", data_fastqs)]\n",
    "\n",
    "# Get the sample name (ID), which is everything before the first \"_\". Adjust this for your file name convention\n",
    "sample_id = function(path) {\n",
    "  full_name = basename(path)\n",
    "  sub(\"_.*\", \"\", full_name)\n",
    "}\n",
    "\n",
    "# Apply ID names to the forward_reads\n",
    "apply_ids = sapply(forward_reads, sample_id)\n",
    "\n",
    "# Since the order of reverse_reads can be different, align in the same order as forward_reads\n",
    "reverse_reads = reverse_reads[ match(apply_ids, sapply(reverse_reads, sample_id)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7bb1d7",
   "metadata": {},
   "source": [
    "## Create samplesheet.csv for input of pipline\n",
    "Finally the samplesheet is created as a data frame with the columns `sample`, `short_reads_fastq_1`, `short_reads_fastq_2` and `long_reads_fastq_1`, which will be empty in our case. \n",
    "\n",
    "It is then saved as a .csv file in the desired location and ready to use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec275539",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samplesheet was created in samplesheet.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify sample IDs together with the FASTQ file paths in a data frame (structure similar to a table for data storage)\n",
    "samplesheet = data.frame(\n",
    "  sample = apply_ids,\n",
    "  short_reads_fastq_1 = forward_reads, # R1 forward reads\n",
    "  short_reads_fastq_2 = reverse_reads, # R2 reverse reads\n",
    "  long_reads_fastq_1 = rep(\"\", length(apply_ids)) # Not present in our data set so one empty per row\n",
    ")\n",
    "\n",
    "# Save as a .csv file\n",
    "sheet_path = file.path(sheet_dir, \"samplesheet.csv\")\n",
    "write.csv(samplesheet, file = sheet_path, row.names = FALSE, quote = FALSE)\n",
    "\n",
    "cat(\"Samplesheet was created in samplesheet.csv\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3162f94",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5f47f",
   "metadata": {},
   "source": [
    "Run **Detaxizer** pipeline (nf-core/detaxizer is the repository) with the command shown in the next block. \n",
    "\n",
    "The Profile test (--profile test) is a configuration profile from the pipeline that uses minimal resources; M3 specifically has the 'm3c' (using M3 HPC) option. The input is prepared above using the samplesheet.csv file, and the output directory (outdir) is populated with content after execution. All files and directories must be specified with the correct path. Ensure you are using the correct version of the pipeline with `-r`, as this can cause problems.\n",
    "\n",
    "Also, when constructing the shell command, it is important to include the parameters “enable_filter” and “filter_trimmed”, as well as “reads_minlength 70”. The filter step must be activated with the first parameter, then it has to be specified that the already trimmed sequences by detaxizer (not the raw sequences) should be used as input for the filtering step (according to https://nf-co.re/detaxizer/1.0.0/parameters). Finally, filtering out reads that are less than 70 base pairs (bp) in length is primordial. To use the parameter “perform_shortread_redundancyestimation” for checking coverage in the metagenome with the Taxprofiler notebook, the k-mer size must be at least 24 bp; otherwise, an error will appear. For this reason, it is very important to include the parameter “reads_minlength_70” in **Detaxizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802af4a3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'cd /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output &amp;&amp; conda activate nextflow &amp;&amp; nextflow run nf-core/detaxizer -r 1.0.0 -profile m3c --input /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output/sheets/samplesheet.csv --outdir /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output/output_detax --filter_trimmed --enable_filter --reads_minlength 70'"
      ],
      "text/latex": [
       "'cd /mnt/lustre/groups/maier/maina479/projects/Detaxizer\\_Notebook\\_final/data/output \\&\\& conda activate nextflow \\&\\& nextflow run nf-core/detaxizer -r 1.0.0 -profile m3c --input /mnt/lustre/groups/maier/maina479/projects/Detaxizer\\_Notebook\\_final/data/output/sheets/samplesheet.csv --outdir /mnt/lustre/groups/maier/maina479/projects/Detaxizer\\_Notebook\\_final/data/output/output\\_detax --filter\\_trimmed --enable\\_filter --reads\\_minlength 70'"
      ],
      "text/markdown": [
       "'cd /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output &amp;&amp; conda activate nextflow &amp;&amp; nextflow run nf-core/detaxizer -r 1.0.0 -profile m3c --input /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output/sheets/samplesheet.csv --outdir /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output/output_detax --filter_trimmed --enable_filter --reads_minlength 70'"
      ],
      "text/plain": [
       "cd /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output && conda activate nextflow && nextflow run nf-core/detaxizer -r 1.0.0 -profile m3c --input /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output/sheets/samplesheet.csv --outdir /mnt/lustre/groups/maier/maina479/projects/Detaxizer_Notebook_final/data/output/output_detax --filter_trimmed --enable_filter --reads_minlength 70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue::glue(\"cd {out_dir} && \\\\\n",
    "conda activate {conda_env} && \\\\\n",
    "nextflow run nf-core/detaxizer -r 1.0.0 \\\\\n",
    "-profile m3c \\\\\n",
    "--input {samples_sheet} \\\\\n",
    "--outdir {pipeline_out} \\\\\n",
    "--filter_trimmed \\\\\n",
    "--enable_filter \\\\\n",
    "--reads_minlength 70\",\n",
    "    out_dir = out_dir,\n",
    "    conda_env = \"nextflow\",\n",
    "    samples_sheet = sheet_path,\n",
    "    pipeline_out = output_detax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23043d1",
   "metadata": {},
   "source": [
    "## Pipeline Output\n",
    "\n",
    "Important folders for further metagenome processing and analysis are the following:\n",
    "- `output_detax/filter`, which is later used in the Taxprofiler Notebook.\n",
    "- `output_detax/multiqc`, specifically the \"multiqc_report.html\" for quality control.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
